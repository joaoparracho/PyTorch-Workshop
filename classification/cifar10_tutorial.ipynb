{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/joaoparracho/PyTorch-Workshop/blob/main/classification/cifar10_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjyQNp07-uAv"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQe9MHz-uAy"
      },
      "source": [
        "Training a Classifier\n",
        "=====================\n",
        "\n",
        "----------------\n",
        "\n",
        "Generally, when you have to deal with image, text, audio or video data,\n",
        "you can use standard python packages that load data into a numpy array.\n",
        "Then you can convert this array into a `torch.Tensor`.\n",
        "\n",
        "-   For images, packages such as Pillow, OpenCV are useful\n",
        "-   For audio, packages such as scipy and librosa\n",
        "-   For text, either raw Python or Cython based loading, or NLTK and\n",
        "    SpaCy are useful\n",
        "\n",
        "`torchvision` has data loaders for common datasets such as ImageNet, CIFAR10,\n",
        "MNIST, etc. and data transformers for images, viz.,\n",
        "`torchvision.datasets` and `torch.utils.data.DataLoader`.\n",
        "\n",
        "For this tutorial, we will use the CIFAR10 dataset. It has the classes:\n",
        "'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
        "'ship', 'truck'. The images in CIFAR-10 are of size 3x32x32, i.e.\n",
        "3-channel color images of 32x32 pixels in size.\n",
        "\n",
        "![cifar10](https://pytorch.org/tutorials/_static/img/cifar10.png)\n",
        "\n",
        "Training an image classifier\n",
        "----------------------------\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1.  Load and normalize the CIFAR10 training and test datasets using\n",
        "    `torchvision`\n",
        "2.  Define a Convolutional Neural Network\n",
        "3.  Define a loss function\n",
        "4.  Train the network on the training data\n",
        "5.  Test the network on the test data\n",
        "\n",
        "### 1. Load and normalize CIFAR10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3EFQzdq-uA1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "\n",
        "class Custom_Dataset(Dataset):\n",
        "    def __init__(self,split=\"train\"):\n",
        "        is_train = True if split == \"train\" or split == \"val\" else False\n",
        "\n",
        "        transform = transforms.ToTensor()\n",
        "        self.dataset = datasets.CIFAR10('data', train=is_train, \n",
        "                                                download=True,\n",
        "                                                transform=transform)\n",
        "                                                \n",
        "        self.indices = np.arange(0,len(self.dataset))\n",
        "        print(self.indices)\n",
        "        # lets use 20% of the trainning dataset to validation\n",
        "        if split == \"train\":\n",
        "            self.indices = self.indices[0:int(len(self.dataset)*0.8)]\n",
        "        elif split == \"val\":\n",
        "            self.indices = self.indices[int(len(self.dataset)*0.8):]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "            image = self.dataset[self.indices[idx]][0]\n",
        "            label = self.dataset[self.indices[idx]][1]\n",
        "            return image, label\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "\n",
        "class Custom_Dataset_Balanced_Split(Dataset):\n",
        "    def __init__(self,split=\"train\"):\n",
        "        is_train = True if split == \"train\" or split == \"val\" else False\n",
        "            \n",
        "        transform = transforms.ToTensor()\n",
        "        self.dataset = datasets.CIFAR10('data', train=is_train, download=True, transform=transform)\n",
        "        \n",
        "        x = np.array(self.dataset.targets)\n",
        "        sorted_indices = np.argsort(x)\n",
        "        num_elem_class = int(np.sum(x==0))\n",
        "        s = np.zeros((10,num_elem_class),int)\n",
        "        for i in range(10):\n",
        "            s[i] = sorted_indices[i*num_elem_class:(i+1)*num_elem_class]\n",
        "\n",
        "        self.sorted_indices = [item for indices in zip(s[0], s[1], s[2],s[3],s[4], s[5],s[6],s[7],s[8],s[9]) for item in indices]\n",
        "        \n",
        "        # lets use 20% of the trainning dataset to validation\n",
        "        if split == \"train\":\n",
        "            self.sorted_indices = self.sorted_indices[0:int(len(self.sorted_indices)*0.8)]\n",
        "        elif split == \"val\":\n",
        "            self.sorted_indices = self.sorted_indices[int(len(self.sorted_indices)*0.8):]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.dataset[self.sorted_indices[idx]][0]\n",
        "        label = self.dataset[self.sorted_indices[idx]][1]\n",
        "        return image, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sorted_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 4   \n",
        "trainset = Custom_Dataset(split=\"train\")\n",
        "valset = Custom_Dataset(split=\"val\")\n",
        "testset = Custom_Dataset(split=\"test\")\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u86ydmM6-uA8"
      },
      "source": [
        "Plot some of the training images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "rPn2HDzv-uA8",
        "outputId": "f0494149-3e95-4670-9f04-789a8d8bf9ad"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yzK5Drm-uA-"
      },
      "source": [
        "### 2. Define the Classifiction model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAIgdRez-uA_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Simple residual block with two 3x3 convolutions.\n",
        "\n",
        "    Args:\n",
        "        in_ch (int): number of input channels\n",
        "        out_ch (int): number of output channels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_ch, kernel_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, in_ch,kernel_size)\n",
        "        self.conv2 =  nn.Conv2d(in_ch, in_ch,kernel_size)\n",
        "        self.conv3 =  nn.Conv2d(in_ch, in_ch,kernel_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.padding = nn.ReflectionPad2d(2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(self.padding(x))\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(self.padding(out))\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = out + identity\n",
        "        return out\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "class Net_2(Net):\n",
        "    def __init__(self):\n",
        "        super(Net_2, self).__init__()\n",
        "        self.fc3 = nn.Linear(84, 42)\n",
        "        self.fc4 = nn.Linear(42, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool((F.relu(self.conv1(x))))\n",
        "        x = self.pool((F.relu(self.conv2(x))))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "class Net_residual(Net):\n",
        "    def __init__(self):\n",
        "        super(Net_residual, self).__init__()\n",
        "        self.res1 = ResidualBlock(6,5)\n",
        "        self.res2 = ResidualBlock(16,5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.res1(F.relu(self.conv1(x))))\n",
        "        x = self.pool(self.res2(F.relu(self.conv2(x))))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye0HXGPU-uBA"
      },
      "source": [
        "### 3. Define a Loss function and optimizer\n",
        "\n",
        "\n",
        "Let\\'s use a Classification Cross-Entropy loss and the Adam optimiser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I87CYaIk-uBB"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed = 0\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "net = Net()\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "accuracy_val_max = 0\n",
        "checkpoint_path = 'cifar_net.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a function to compute the model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(x,labels_x):\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(x.data, 1)\n",
        "    correct = (predicted == labels_x).sum().item()\n",
        "    return correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPx5GH_2-uBC"
      },
      "source": [
        "### 4. Train the network\n",
        "\n",
        "\n",
        "Now it's fun time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "bijYCRxr-uBD",
        "outputId": "626a9e74-97ac-4ec9-c777-3b8fdfbad26e"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(net)\n",
        "print(device)\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        \n",
        "        correct += accuracy(outputs,labels)\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch }, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "            \n",
        "    print(f'Accuracy of the network on training set: {100 * correct / total} %')\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            \n",
        "            images, labels = data\n",
        "            images=images.to(device)\n",
        "            labels=labels.to(device)\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(images)\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += accuracy(outputs,labels)\n",
        "            # correct += accuracy(outputs,labels)\n",
        "    \n",
        "    accuracy_val = correct / total\n",
        "    \n",
        "    if accuracy_val > accuracy_val_max:\n",
        "        accuracy_val_max = accuracy_val\n",
        "        torch.save(net.state_dict(), checkpoint_path)\n",
        "        best_epoch = epoch\n",
        "            \n",
        "    print(f'Accuracy of the network on validation set: {np.round(100 * accuracy_val,3)} %')\n",
        "\n",
        "print(\"Best epoch: \"+str(epoch))\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba0kumF7-uBF"
      },
      "source": [
        "See [here](https://pytorch.org/docs/stable/notes/serialization.html) for\n",
        "more details on saving PyTorch models.\n",
        "\n",
        "### 5. Inference on the test data\n",
        "\n",
        "\n",
        "Check if the network has learnt anything at all.\n",
        "\n",
        "This is checked by predicting the class label that the neural network\n",
        "outputs, and checking it against the ground-truth. If the prediction is\n",
        "correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Let us, first, display an image from the test set to get\n",
        "familiar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "xISXRfha-uBF",
        "outputId": "88ce25ee-0dd8-4447-94f3-cebef8dc852a"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHa_vhH2-uBG"
      },
      "source": [
        "Load the saved model (note: saving and re-loading\n",
        "the model wasn\\'t necessary here, it's only to illustrate how to do so):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWB-ztaW-uBG",
        "outputId": "715ef5f7-6f0e-4f30-d1dc-d29a1af0c095"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "net = Net()\n",
        "net.to(device)\n",
        "print(checkpoint_path)\n",
        "net.load_state_dict(torch.load(checkpoint_path))\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na6DDcEy-uBI"
      },
      "source": [
        "The outputs of the model are energies for the 10 classes. \n",
        "\n",
        "The higher the energy for a class, the more the network thinks that the image is of the particular\n",
        "class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_yKosg-uBK"
      },
      "source": [
        "Let us look at how the network performs on the whole dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4C3iNpH-uBK",
        "outputId": "a8b42bce-88b7-4980-9c8e-dde769d5c069"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images=images.to(device)\n",
        "        labels=labels.to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw3rxnkv-uBL"
      },
      "source": [
        "Performance per class:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "hH4bEk2e-uBL",
        "outputId": "16557a96-a213-46de-f173-4dae6eb6bcea"
      },
      "outputs": [],
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images=images.to(device)\n",
        "        labels=labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJppAj3O-uBM"
      },
      "source": [
        "So what next?\n",
        "\n",
        "Now, try to improve the classification performance of the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8XNjuTZ-uBM"
      },
      "source": [
        "**Tips** : \n",
        "\n",
        "-   Try to use more epochs\n",
        "-   Increase the complexity of your network by adding more channels to the convolutional layers.\n",
        "-   Explore different learning rates. (Change lr argument in the Adam optimiser)\n",
        "-   Try different models (e.g Net_2(), Net_residual())\n",
        "-   Try different pre-processing techiques (e.g flipping)\n",
        "\n",
        "**Goals achieved**:\n",
        "\n",
        "-   Understanding PyTorch\\'s Tensor library and neural networks at a\n",
        "    high level.\n",
        "-   Train a small neural network to classify images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8c_HqzC-uBN"
      },
      "outputs": [],
      "source": [
        "del dataiter"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
